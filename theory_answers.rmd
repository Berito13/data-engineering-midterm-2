1. **Redis-ის შესახებ**:
   1) strings - რიცხვების, ტექსების, json ფორმატების შესანახად გამოიყენება
   2) hash - key-value წყვილების შესანახად. გამოიყენება მაგალითად იუზერების ინფორმაციის შესანახად.
   3) list - მონაცემთა მწკრივების შესანახად გამოიყენება. მაგალითად ჩაწერილი ოპერაციები რიგში
   4) set - როგორც ზოგადად ინახავს დუბლიკატების გარეშე მონაცემებს. მომხმარებელზე რაღაც ინფორმაციის შესანახად - მაგალითად იმეილების.
   5) sorted set - დალაგებული set. მონაცემები სტუდენტების ქულების მიხედვით.

2. **Apache Kafka-ს შესახებ**
Broker - Kafka Broker არის სერვერი, რომელიც იღებს, ინახავს და აწვდის შეტყობინებებს
Topic - Kafka-ში მონაცემები ინახება Topic-ებში
Partition - თითოეული Topic იყოფა რამდენიმე Partition-ად, რაც Kafka-ს მასშტაბირებადობას უზრუნველყოფს
Consumer Groups - Consumers კითხულობენ მონაცემებს Topic-ებიდან, ხოლო Consumer Group-ები თანაბრად ანაწილებენ ამ დატვირთვას

Partitioning - გამოყენებით kafka ანაწილებს მონაცემეს მრავალ bkorek-ს შორის და ხდება მონაცემთა პარალელური დამუშვება
Kafka-ს Consumer Group-ები ავტომატურად ანაწილებენ partition-ებს, რაც ამცირებს გადატვირთვას.
Kafka იმეორებს (Replicates) მონაცემებს სხვა Broker-ებზე, რომ მონაცემების დაკარგვის თავიდან აცილება მოხდეს.
Kafka Producers აგზავნიან მონაცემებს ასინქრონულად, რაც ზრდის წარმადობას
Consumers პარალელურად კითხულობენ მონაცემებს, თითოეულ Partition-ს დამოუკიდებლად

3. **Apache Airflow-ს შესახებ**:
   airflow გვეხმარება, ავტომატურად გავუშვათ სხვადასხვა სახის ჯობები და etl პრიცესები.
ჯობებში იგულისხმება მაგალითად მეილების დაგზავნა, ფაილების დამუშვება და აშ. etl pipeline აგებაც ხდება, სადაც შეგვიძლია დამოკიდებული რგოლები გამოვნახოთ და ერთმანეთის მიყოლებით გავაკეთებინოთ

Operators არის მთავარი კომპონენტები, რომლებიც შესრულებად დავალებებს (Tasks) განსაზღვრავენ DAG-ში.

Sensors მუშაობენ მონიტორინგის პრინციპით და ამოწმებენ, შესრულდა თუ არა კონკრეტული მოვლენა.
მაგალითად API -სთან კავშირი ან სხვა.

4. **ETL vs ELT**
   ETL  - არის extract transform load - გამოიყენება ისეთი ინმოფრმაციების დამუშვების დროს როცა შეგვიძლია იფნროამცია უკვე დამუშვებულად ჩავწეროთ. მაგალითად დაშიფრული ბაზებიდან ინფორმაციის წამოღებისას შეგვიძლია ჯერ დეშიფრაცია გავუკეთოთ და შემდეგ ჩავწეროთ. ამის მაგალითად შემიძლია მოვიყვანო ჩემს მიერ შესრულებული დავალება 1C პროგრამის ბაზებთან მუშაობის დროს ხშირად ვიყენებთ რადგან მოვაცილოთ არასასურველი ინფორმაცია და რეპორტინგი უკვე გამზადებულ ცხრილებზე ავაწყოთ.

    ELT - არის extract load transform - ამის მაგალითად მოვიყვან სალაროდან წამოსული გაყიდვების ჩაწერას. პირდაპირ ვადგებით ბაზის ცხრილს და მოგვაქვს, და ამიშ შემდეგ იწყება მისი ტრანსოფრმაცია და აგრეგაციების კეთება სხვადასხვა ცხრილებში.

    etl - პროცესის დადებითი მხარე ჩემი აზრით არის ის რომ ინფორმაცია უკვე გალამზებულია და აღარ მოითხოვს დამატებით დამუშვებას, თუმცა ნაკლოვანებაში შეიძლება ვთქვათ ახალის ვეტის დამატების დროს დიდი სამუშაო იქნება გასაწევი დიდ ინფორმაციაზე მუშაობისას

    elt - ნაკლებ ოპერაციას ვაკეთებთ წამოღებულინფორმაციაზე და ყველაფერი მოგვაქვს და ამიტომ არ დაგვწირდება შემდეგ სხვადასხვა სვეტების დამატება, თუმცა ნაკლოვანებაში შეიძლება ჩავთვალოთ დიდი ზომის ინფორმაციის არსებობა.

5. **მონაცემთა შენახვის კონცეფციები**
Data Lake - არის დიდი ზომის მონაცემთა საცავი, სადაც ინფორმაცია იწერება სტრუქტურირებულ და არასტრუქტურირებულ მდგომარეობაში,  დაუმუშავებელ მდგომარეობაში და გამოიყენება მონაცემების შესახად და ანალიზისთვის

Data Warehouse - არის სტრუქტურირებული მონაცემთა საცავი რომელიც გამოიყენე Bi tool-ში ინტეგრაციისთვის და შემდეგ ანალიტიკაში

Data Mart არის data warehousis შვილი, რომელიც განკუთვნილია კონკრეტული დეპარტამენტისთვის

1️ Data Lake  გამოიყენებს ELT-ს, რადგან მონაცემები თავიდან იტვირთება ნედლ მდგომარეობაში და გარდაიქმნება საჭიროების მიხედვით.
2️ Data Warehouse  უფრო მეტად ემყარება ETL-ს, რადგან მონაცემები უნდა იყოს კარგად სტრუქტურირებული ჩატვირთვამდე.
3️ Data Mart  იღებს მონაცემებს Data Warehouse-დან ETL პროცესის მეშვეობით, რათა კონკრეტული დეპარტამენტისთვის მოამზადოს ინფორმაცია.





